The Colorado Undergraduate Space Research Symposium is an opportunity for students to present their current research and ideas in space related fields to their peers, industry and academia.  Four schools from around Colorado submitted papers this year and presented their research during the Symposium.  Teams and individuals, freshman through seniors, have researched a variety of topics and presented their research or practical engineering projects.

    Paper - Technical Content, Scientific Merit and Originality (45%)

    Paper - Practical Application (15%)

    Paper - Clarity of Writing (10%)

    Presentation - Visual and Oral presentation and response to questions (30%)


---------------------------SCOTT's OUTLINE:--------------------------------------------

    Problem

        Current State

        Notable Challenges

            Autonomous Benefit with Low Communications

    Current Solutions

        Trends

        Alternative Solutions (to our solution) [The Modern Day answer]

            Strengths/Weakness [Cost-benefit balancing act]

        Our Solution

            Costs/Weaknesses

            Benefits/Strengths

    Expand Into Methodology [Proof of Concept]

        Price/Power Point and Energy Consumption

        Tech Talk [Humanize]

        Features/Advantages/Benefits

    Scalability/Closing

        Implications of Proof of Concept

        Where it could be [Make it real to them]


    Pathfind

        Characterizing

            The problem appears 2D -> Actually 3d [Traverse Log at 90 degrees]

        Development

            Probabilistic/monte-carlo

                Difficult in 3d -- problem becomes Huge and Unweildly

            Pre Determined

                Lack of flexibility in dynamic environment

            Hybrid

                Predetermined paths with on the fly course correction

                Demanded a GUI interface

                Situational Awareness [Properties of envirionment and path]

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++      MAX's CONTRIBUTIONS         +++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++



Background:

Forest fires present a particular challenge in land management.  The economic impact of these fires can be severe, but  supression of fires can result in long-term negative effects on forest ecosystems, as well as create the potential for more damaging fires.  Efforts to manage these effects are hampered by a lack of data about forest ecosystems and by the need to improve understanding of large-scale forest disturbances[Maclean, Flores].  

One promising avenue for the development of forest land management strategies are geostatistical surveys, including geostatistical soil analysis.  Geostatistical methods of soil analysis have been in use for nearly six decades.  The aim of these techniques is to use a small number of soil samples from discrete points spread out across a geographical area to interpolate information about areas between these points.  This form of analysis is useful in implementing precision agriculture and in conducting environmental assessments [Mason, Olea. USDA].

Geostastical forest surveys face several main obstacles.  Firstly, the cost of taking samples distributed across a wide geographic area is often prohibitive.  The sampling locations must neccesarily include remote regions, and measurements at multiple points in time are recommended [USDA].  Secondly, the forest must be surveyed before a fire[Flores], so effective surveys need to cover very large areas.  Finally, knowledge about a forest system is fundamentally constrained by the number of sampling locations [Olea].  These factors together limit the effectiveness of each survey for a given cost, reducing the utility of an otherwise plausible methodology for investigation.

These costs can all be reduced by the introduction of an automated system that can collect samples and perform measurements [Sadowsky].  Automated systems thus far have been limited to those which require direct human implementation at the location of the soil sample.  In this paper, we present the Metropolitain State University of Denver Autonomous Soil Investigator, a proof-of-concept robotic system which could navigate a forest landscape and perform the collection of soil samples.  This robotic system can function autonomously and costs less that $2000 in raw materials.  The development and implementation of this robotic system has the potential to dramatically reduce the cost of sampling efforts while also improving the geographic density and frequency of data collection, providing forest researches with a powerful avenue for understanding and managing forest fires.


______________________________

Refs:

Proceedings of the Second International Symposium on Fire Economics, Planning, and Policy: A Global View - Reconstruction of Forest Areas Using Geostatistics as an Aid in the Evaluation of Burned Areas J. Germán Flores G., David A. Moreno G., Francisco Rincón R.
    -another excellent discussion of geostatistics and fire management 

Automated Robotic Assay of Phosphomonoesterase Activity in Soils
M. J. Sadowsky,* W. C. Koskinen, J. Seebinger, B. L. Barber, and E. Kandeler
Sci. Soc. Am. J. 70:378–381 (2006).
    -Basic robotic technique for on-the-spot analysis

Olea, R.A., 2009, A practical primer on geostatistics: U.S. Geological Survey, Open-
File Report 2009-1103, 346 p.
    -An overview of geostatistical methods

Maclean, Ann L., and David T. Cleland. "The use of geostatistics to determine the spatial extent of historical fires as an aid in understanding fire regimes for northern lower Michigan." Proceedings on the symposium on fire, fuel treatments, and ecological restoration. Proc. RMRS-P-29, Fort Collins, Colorado, USA. 2003.
    -Excellent discussion of geostatistics as a tool for fire management

Soil Survey Division Staff. 1993. Soil survey manual. Soil Conservation Service. U.S. Department of Agriculture Handbook 18.
    -A good discussion of the history + nuts and bolts of soil surveying.  Aimed at agriculture but principles apply to forestry.


    _________________________________________________________________________________________________
           _____________________________________________________________________________________            
                    ______________________________________________________                   ________________________________________________________________________________________________________________________            


Localization:

Localization is conducted with data from four "eye" modules, located on the four corners of the robot.  The modules consist of an infrared triangulator, an ultrasound sonar unit, and a precision servo.  The servos control the other sensors as they sweep out 145 degree fan-shaped regions about the robot, returning ranges to nearby objects.

Our robot implements a Monte-Carlo multiple hypothesis belief approach to localization.  The robot's pose is considered as a probability distribution in {x,y, theta} space, where x and y are the distances from the center of the robot to the edges of the board, and theta is the angle between the robot's heading and the x axis.   This probability distribution is represented as a cloud of discrete hypotheses, or "hypobots," each with its own weight.  When the weights are normalized so they total one across all hypobots, then each hypobot's weight is approximately the probability that the hypobot's pose is the most correct of any in the cloud.

At the beginning of the round, the hypothetical poses (or "hypobots") are arranged in a dense flat distribution across the starting region, each with weight 1.  Each As the robot moves, the hypobots mimic the expected motion of the robot through its' pose space, with stochastic adjustments in x, y and theta at fixed time intervals.  The cloud of hypobots then resembles a collection of particles diffusing across the map. 

When the standard deviation in the cloud of hypobots becomes too large, or if the pathfinding algorithm deems it neccessary, the robot performs a scan.  As the eye modules rotate, the robot simulates ideal measurements from the eye modules for each hypothesis of its pose.   Each hypobot then stores the results, populating lists with a simulated datapoint for each real data point that the physical robot measured.   

Each hypobot compares each real data point with its corresponding simulated data point, and multiplies its weight by the approximate bayseian probabilty of its' own correctness given that data point.  This probability is taken as a gaussian function of the difference between the real and ideal data point using an empirically determined, non-linear sigma. This process is equivalent to applying a one-step extended Kalman filter to each hypobot.

Once all the hypobots are weighted, the weights are normalized and a weighted average pose is taken as the best guess pose.  This result is passed to the pathfinder, but the hypobot cloud is retained.  Hypobots with very low weights are discarded and new hypobots are cloned from the remaining cloud.  This avoids wasting computational time on very unlikely hypobots while still maintaining a dense cloud.  

This process has the advantages of being robust and adaptable, maintaining its effectiveness in a variety of maps.  This adaptibility comes at the cost of computational time, but this can be mitigated by performing the expensive simulation calculations during the scan.



